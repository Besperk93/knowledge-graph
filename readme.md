# Dissertation Project #

This is the repository for the project undertaken in order to fulfil the rquirements of the MSc Computing dissertation. The repository is structured roughly as follows:

 - Vault
 - scrapeKhan
 - extractInfo
 - augmentModel

_Vault_

The vault is ignored by git and contains all files and outputs to be stored locally. This is where model checkpoints and large datasets can be stored without being pushed to GitHub. The subdirectory is often the target of function and model outputs and this should be the first place one looks for any generated objects. As such it tends to pick up a lot of debris.

_scrapeKhan_

This subdirectory contains the code used to scrape the Khan Academy youtube channel and prodcuce the initial store of video transcripts that the project draws upon. As a purely academic pursuit this use of the Khan Academy video and the YouTube generated subtitles conforms with the relevant CC-BY-NC-SA license (https://www.khanacademy.org/about/tos).

_extractInfo_

This subdirectory contains the code used to train the BERT match the blanks model and use it to extract entities and relations from the video transcripts generated by the scrapeKhan code. The output of this section should be the component parts of our knowledge graph.

Some of the training procedures in this section drew heavily from code made available by Wee Tee Soh (https://github.com/plkmo/BERT-Relation-Extraction) and credit is given whenever relevant. Annotated references from the original repository, which were used to adapt and extend this code, are also provided where relevant. Annotations that are mine begin with "NOTE:".


_augmentModel_

This subdirectory contains the code used to integrate the knowledge graph created in the previous section with a knowledge attention and recontextualisation layer, as outlined by Peters et al, 2019. The model draws upon the paper's repository (https://github.com/allenai/kb) as well as code written by Niclas Doll (https://github.com/ndoll1998/KnowBert). Credit is given to both when relevant.

Here the KAR layer is adapted to work with a GPT2 model provided by Hugging Face in their transformers library. The resulting KnowGPT2 model is then used to generate answers from the MATH dataset.
